{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d019ceb-2b2f-4d1a-a0af-c6dbf2acae0a",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "added_path = os.path.join(os.path.abspath(\"..\"), \"VAUB-gp\")\n",
    "if added_path not in sys.path:\n",
    "    sys.path.append(added_path)\n",
    "# print(sys.path)\n",
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from synthetic_exp_util import get_dataloader, calculate_auroc, vae_loss, calculate_gp_loss, Score_fn, UNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd0de459be732c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2 * hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)  # Mean and log-variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2 * hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.bn_mean = nn.BatchNorm1d(latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mean, logvar = h.chunk(2, dim=-1)\n",
    "        mean = self.bn_mean(mean)\n",
    "        logvar = torch.clamp(logvar, max=4)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z), z, mean, logvar\n",
    "    \n",
    "    def init_weights(self, scale=0.1):\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, a=-scale, b=scale)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    \n",
    "def train_vaub_gp(mode, device, is_vanilla, input_dim, latent_dim, alpha, loops, hidden_dim, timesteps, sigma_max,\n",
    "                  sigma_min, lr_vae, lr_score, beta, gp_lambda, num_epochs, dataloader_domain1, dataloader_domain2,\n",
    "                  dataloader_score1, dataloader_score2, num_visual=1, num_log=1, plot=True):\n",
    "\n",
    "    vae1 = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "    vae2 = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "    # vae1.init_weights()\n",
    "    # vae2.init_weights()\n",
    "    score_model = Score_fn(UNet(in_dim=2, out_dim=2, num_timesteps=timesteps, is_warm_init=False), sigma_min=sigma_min, sigma_max=sigma_max, num_timesteps=timesteps, device=device).to(device)\n",
    "    optimizer_vae = optim.Adam(chain(vae1.parameters(), vae2.parameters()), lr=lr_vae)\n",
    "    optimizer_score = torch.optim.Adam(score_model.parameters(), lr=lr_score)\n",
    "\n",
    "    total_loss_list = []\n",
    "    recon_loss_list = []\n",
    "    kl_loss_list = []\n",
    "    gp_loss_list = []\n",
    "    # Training\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        vae1.train()\n",
    "        vae2.train()\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_kld_encoder_posterior = 0\n",
    "        total_kld_prior = 0\n",
    "\n",
    "        for i, (data1, data2) in enumerate(zip(dataloader_domain1, dataloader_domain2)):\n",
    "            x1, label1 = data1\n",
    "            x2, label2 = data2\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            optimizer_vae.zero_grad()\n",
    "\n",
    "            recon_x1, z1, mean1, logvar1 = vae1(x1)\n",
    "            recon_x2, z2, mean2, logvar2 = vae2(x2)\n",
    "            x, recon_x, z, mean, logvar = torch.vstack((x1, x2)), torch.vstack((recon_x1, recon_x2)), torch.vstack((z1, z2)), torch.vstack((mean1, mean2)), torch.vstack((logvar1, logvar2))\n",
    "\n",
    "            # DSM = score_model.get_LSGM_loss(z, is_mixing=True, is_residual=True, is_vanilla=is_vanilla)\n",
    "            score = score_model.get_mixing_score_fn(z, 5*torch.ones(z.shape[0], device=device).type(torch.long), detach=True, is_residual=True, is_vanilla=is_vanilla, alpha=alpha) - 0.05 * z\n",
    "            score = torch.matmul(score.unsqueeze(1), z.unsqueeze(-1)).sum()\n",
    "            # score = -torch.sqrt(torch.matmul(score.unsqueeze(1), z.unsqueeze(-1)).sum()**2)\n",
    "\n",
    "            if mode == 'Gaussian':\n",
    "                loss, recon_loss, kld_encoder_posterior, kld_prior = vae_loss(recon_x, x, mean, logvar, beta, score=None, DSM=None)\n",
    "            else:\n",
    "                loss, recon_loss, kld_encoder_posterior, kld_prior = vae_loss(recon_x, x, mean, logvar, beta, score=score, DSM=None)\n",
    "\n",
    "            # dist_func_x = get_lp_dist(p=2)\n",
    "            # dist_func_z = get_lp_dist(p=2)\n",
    "            # gp_loss = sum([compute_gp_loss(x, z, dist_func_x, dist_func_z) for x, z in zip([x1, x2], [z1, z2])])\n",
    "            gp_loss = gp_lambda * calculate_gp_loss([x1, x2], [z1, z2])\n",
    "\n",
    "            gp_loss_list.append(gp_loss.item())\n",
    "\n",
    "            loss += gp_loss\n",
    "            total_loss_list.append((loss).item())\n",
    "            recon_loss_list.append((recon_loss).item())\n",
    "            kl_loss_list.append((kld_encoder_posterior+kld_prior).item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_vae.step()\n",
    "\n",
    "            # if epoch % 25 == 0 and i==0:\n",
    "            #     print(f'score loss: {score}')\n",
    "            #     print(f'LSGM loss: {DSM}')\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kld_encoder_posterior += kld_encoder_posterior.item()\n",
    "            total_kld_prior += kld_prior.item()\n",
    "\n",
    "            # Update Score Function\n",
    "            for loop in range(loops):\n",
    "                data1, data2 = next(iter(zip(dataloader_score1, dataloader_score2)))\n",
    "                x1, label1 = data1\n",
    "                x2, label2 = data2\n",
    "                x1, x2 = x1.to(device), x2.to(device)\n",
    "                recon_x1, z1, mean1, logvar1 = vae1(x1)\n",
    "                recon_x2, z2, mean2, logvar2 = vae2(x2)\n",
    "                x, recon_x, z, mean, logvar, labels = torch.vstack((x1, x2)), torch.vstack((recon_x1, recon_x2)), torch.vstack((z1, z2)), torch.vstack((mean1, mean2)), torch.vstack((logvar1, logvar2)), torch.vstack((label1, label2))\n",
    "                # print(loop)\n",
    "                if loop == (loops-1) and (epoch+1) % (num_epochs//num_visual) == 0 and i==0:\n",
    "                    # print(f\"Epoch {epoch} DSM average loss:\", end=' ')\n",
    "                    recon_x1_z2 = vae1.decode(z2)\n",
    "                    import seaborn as sns\n",
    "                    \n",
    "                    # Set up seaborn style for improved aesthetics\n",
    "                    sns.set(style='white')\n",
    "                    \n",
    "                    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "                    \n",
    "                    # X1 Given Z2 plot\n",
    "                    ax[0].scatter(recon_x1_z2.detach().cpu()[:, 0], recon_x1_z2.detach().cpu()[:, 1], marker='.', color='darkblue', s=30)\n",
    "                    # ax[0].set_title('Translated x1 given z2', fontsize=14)\n",
    "                    # ax[0].set_xlabel('Dimension 1', fontsize=12)\n",
    "                    # ax[0].set_ylabel('Dimension 2', fontsize=12)\n",
    "                    \n",
    "                    # X1 Reconstruction plot\n",
    "                    ax[1].scatter(recon_x1.detach().cpu()[:, 0], recon_x1.detach().cpu()[:, 1], marker='.', color='darkgreen', s=30)\n",
    "                    # ax[1].set_title('x1 reconstructed', fontsize=14)\n",
    "                    # ax[1].set_xlabel('Dimension 1', fontsize=12)\n",
    "                    # ax[1].set_ylabel('Dimension 2', fontsize=12)\n",
    "                    \n",
    "                    # Data split scatter plot\n",
    "                    n_samples = x1.shape[0] if x1.shape[0] < 200 else 200\n",
    "                    data = z.detach().cpu()\n",
    "                    data1, data2 = data.chunk(2)\n",
    "                    labels1, labels2 = labels.view((-1,)).chunk(2)\n",
    "                    \n",
    "                    # Data subsets by labels\n",
    "                    data1_l1, data1_l2 = data1[labels1 == 0], data1[labels1 == 1]\n",
    "                    data2_l1, data2_l2 = data2[labels2 == 0], data2[labels2 == 1]\n",
    "                    \n",
    "                    # Plot scatter with different markers and labels\n",
    "                    ax[2].scatter(data1_l1[:n_samples, 0], data1_l1[:n_samples, 1], marker='+', label='D1_L1', c='b', s=40)\n",
    "                    ax[2].scatter(data1_l2[:n_samples, 0], data1_l2[:n_samples, 1], marker='o', label='D1_L2', c='b', s=40, edgecolors='k')\n",
    "                    ax[2].scatter(data2_l1[:n_samples, 0], data2_l1[:n_samples, 1], marker='+', label='D2_L1', c='g', s=40)\n",
    "                    ax[2].scatter(data2_l2[:n_samples, 0], data2_l2[:n_samples, 1], marker='o', label='D2_L2', c='g', s=40, edgecolors='k')\n",
    "                    \n",
    "                    # Add title, labels, legend\n",
    "                    # ax[2].set_title('Latent Space Representation', fontsize=14)\n",
    "                    # ax[2].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "                    # ax[2].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "                    ax[2].legend(fontsize=10, loc='upper right')\n",
    "                    \n",
    "                    # Improve layout and space between subplots\n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # Show the plot\n",
    "                    plt.show()\n",
    "\n",
    "                    \n",
    "                                        \n",
    "                    score_model.update_score_fn(z, optimizer=optimizer_score, max_timestep=None, verbose=True, is_mixing=True, is_residual=True, is_vanilla=is_vanilla, alpha=alpha)\n",
    "                else:\n",
    "                    score_model.update_score_fn(z, optimizer=optimizer_score, max_timestep=None, is_mixing=True, is_residual=True, is_vanilla=is_vanilla, alpha=alpha)\n",
    "        # Print every 25 epochs\n",
    "        if (epoch + 1) % (num_epochs//num_log) == 0:\n",
    "            print(f'Epoch {epoch+1}, Total Loss: {total_loss:.2f}, Recon Loss: {total_recon_loss:.2f}, '\n",
    "                  f'Encoder Posterior Loss: {total_kld_encoder_posterior:.2f}, Prior Loss: {total_kld_prior:.2f}, '\n",
    "                  f'Gp loss: {gp_loss}')\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))  # Adjust figsize as needed\n",
    "\n",
    "        # Plot Data and Set Titles\n",
    "        axs[0].plot(total_loss_list, label='total lost')\n",
    "        axs[0].set_title('total lost')\n",
    "\n",
    "        axs[1].plot(recon_loss_list, label='recon list')\n",
    "        axs[1].set_title('recon list')\n",
    "\n",
    "        axs[2].plot(kl_loss_list, label='kl list')\n",
    "        axs[2].set_title('kl list')\n",
    "\n",
    "        axs[3].plot(gp_loss_list, label='gp list')\n",
    "        axs[3].set_title('gp list')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return (vae1, vae2), score_model, z, labels"
   ],
   "id": "b8432d3e4695eab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 50",
   "id": "66255432bf6b6732"
  },
  {
   "cell_type": "code",
   "id": "3837481a-1e5a-49bc-8418-d9df7e34803b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Get data\n",
    "n_points = 50\n",
    "batch_size = 50\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.25\n",
    "gp_lambda = 500\n",
    "num_epochs = 500\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4eb8564-9332-49ce-a839-4dc88f927bab",
   "metadata": {},
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 5e-2\n",
    "lr_score = 2e-3\n",
    "beta = 2\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 1000\n",
    "\n",
    "# Get data\n",
    "n_points = 50\n",
    "batch_size = 50\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(1):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 100",
   "id": "ae632d6e06e7619a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get data\n",
    "n_points = 100\n",
    "batch_size = 100\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.5\n",
    "gp_lambda = 500\n",
    "num_epochs = 500\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "c5005a3d7e45d81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 5e-2\n",
    "lr_score = 2e-3\n",
    "beta = 2\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "# Get data\n",
    "n_points = 100\n",
    "batch_size = 100\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(1):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68c5bab3adc83144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 200",
   "id": "2fdf24df5d787512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get data\n",
    "n_points = 200\n",
    "batch_size = 200\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.5\n",
    "gp_lambda = 200\n",
    "num_epochs = 500\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "6cef7b8a625ed2b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 1\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "# Get data\n",
    "n_points = 200\n",
    "batch_size = 200\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(5):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=False)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "dc296681dd791564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 500",
   "id": "3cda6da4735bb13d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get data\n",
    "n_points = 500\n",
    "batch_size = 500\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.25\n",
    "gp_lambda = 200\n",
    "num_epochs = 500\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "40ac3fdab4da7ff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.4\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "# Get data\n",
    "n_points = 500\n",
    "batch_size = 500\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(1):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=False)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "cecd5b7811c04a52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 10",
   "id": "ce45923fe35bd98c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get data\n",
    "n_points = 20\n",
    "batch_size = 20\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.25\n",
    "gp_lambda = 500\n",
    "num_epochs = 500\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "16c7cf2498626ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.5\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "# Get data\n",
    "n_points = 20\n",
    "batch_size = 20\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(1):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "9e259d6091eac972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# num_samples = 1000",
   "id": "15e46e107739258b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get data\n",
    "n_points = 1000\n",
    "batch_size = 1000\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "# default value\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.5\n",
    "gp_lambda = 200\n",
    "num_epochs = 1000\n",
    "\n",
    "# train the model\n",
    "for _ in range(1):\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"score\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "54c8e33598cc44a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# default value\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_vanilla = True\n",
    "input_dim = 2\n",
    "latent_dim = 2\n",
    "alpha = None\n",
    "\n",
    "# score related\n",
    "loops = 1\n",
    "hidden_dim = 24\n",
    "timesteps = 200\n",
    "sigma_max = 0.4\n",
    "sigma_min = 0.01\n",
    "\n",
    "# rest hyperparams\n",
    "lr_vae = 1e-2\n",
    "lr_score = 2e-3\n",
    "beta = 0.8\n",
    "gp_lambda = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "# Get data\n",
    "n_points = 1000\n",
    "batch_size = 1000\n",
    "dataloader_domain1, dataloader_domain2, dataloader_score1, dataloader_score2 = get_dataloader(n_points=n_points, batch_size=batch_size, plot=False)\n",
    "\n",
    "for _ in range(1):\n",
    "    # train the model\n",
    "    (vae1, vae2), score_model, z, labels = train_vaub_gp(mode=\"Gaussian\", device=device, is_vanilla=is_vanilla, input_dim=input_dim, latent_dim=latent_dim, alpha=alpha, loops=loops, hidden_dim=hidden_dim, timesteps=timesteps, sigma_max=sigma_max, sigma_min=sigma_min, lr_vae=lr_vae, lr_score=lr_score, beta=beta, gp_lambda=gp_lambda, num_epochs=num_epochs, dataloader_domain1=dataloader_domain1, dataloader_domain2=dataloader_domain2, dataloader_score1=dataloader_score1, dataloader_score2=dataloader_score2, plot=True)\n",
    "    \n",
    "    auc_score = calculate_auroc(z, labels)\n",
    "    print(f\"{auc_score:.2f}\")"
   ],
   "id": "2d32a5342f53c582",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "786b6e8a1d0c851d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
